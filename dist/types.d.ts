/**
 * @file types.ts
 * @description Core TypeScript interfaces and types for the LLM Rotation Server
 * Provides OpenAI API compatible interfaces, server configuration types,
 * and error handling types while extending types from llm_rotation.ts
 */
import type { Message, ModelConfiguration, ApiKeys, LlmSettings, ApiKeyStatus, Provider } from './llm_rotation';
export type { Message, ModelConfiguration, ApiKeys, LlmSettings, ApiKeyStatus, Provider };
/**
 * OpenAI Chat Completion Request interface
 * Compatible with OpenAI API v1/chat/completions endpoint
 */
export interface ChatCompletionRequest {
    /** ID of the model to use */
    model: string;
    /** A list of messages comprising the conversation so far */
    messages: Message[];
    /** What sampling temperature to use, between 0 and 2 */
    temperature?: number;
    /** The maximum number of tokens to generate in the chat completion */
    max_tokens?: number;
    /** An alternative to sampling with temperature, called nucleus sampling */
    top_p?: number;
    /** Whether to stream back partial progress */
    stream?: boolean;
    /** A unique identifier representing your end-user */
    user?: string;
    /** Up to 4 sequences where the API will stop generating further tokens */
    stop?: string | string[];
    /** Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency */
    frequency_penalty?: number;
    /** Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far */
    presence_penalty?: number;
    /** Modify the likelihood of specified tokens appearing in the completion */
    logit_bias?: Record<string, number>;
}
/**
 * OpenAI Chat Completion Response interface
 */
export interface ChatCompletionResponse {
    /** A unique identifier for the chat completion */
    id: string;
    /** The object type, which is always "chat.completion" */
    object: 'chat.completion';
    /** The Unix timestamp (in seconds) of when the chat completion was created */
    created: number;
    /** The model used for the chat completion */
    model: string;
    /** A list of chat completion choices */
    choices: ChatCompletionChoice[];
    /** Usage statistics for the completion request */
    usage: ChatCompletionUsage;
}
/**
 * Individual choice in a chat completion response
 */
export interface ChatCompletionChoice {
    /** The index of the choice in the list of choices */
    index: number;
    /** A chat completion message generated by the model */
    message: Message;
    /** The reason the model stopped generating tokens */
    finish_reason: 'stop' | 'length' | 'content_filter' | 'tool_calls' | 'function_call';
}
/**
 * Usage statistics for the completion request
 */
export interface ChatCompletionUsage {
    /** Number of tokens in the prompt */
    prompt_tokens: number;
    /** Number of tokens in the generated completion */
    completion_tokens: number;
    /** Total number of tokens used in the request (prompt + completion) */
    total_tokens: number;
}
/**
 * OpenAI Models List Response interface
 */
export interface ModelsListResponse {
    /** The object type, which is always "list" */
    object: 'list';
    /** List of model objects */
    data: ModelObject[];
}
/**
 * Individual model object in the models list
 */
export interface ModelObject {
    /** The model identifier, which can be referenced in the API endpoints */
    id: string;
    /** The object type, which is always "model" */
    object: 'model';
    /** The Unix timestamp (in seconds) of when the model was created */
    created: number;
    /** The organization that owns the model */
    owned_by: string;
    /** List of permissions for the model */
    permission: any[];
    /** The root model identifier */
    root: string;
    /** The parent model identifier */
    parent: string | null;
}
/**
 * Server configuration interface
 */
export interface ServerConfig {
    /** Port number for the server to listen on */
    port: number;
    /** API keys for different providers */
    apiKeys: ApiKeys;
    /** Default settings for LLM requests */
    defaultSettings: DefaultLlmSettings;
    /** Environment (development, production, etc.) */
    environment?: string;
    /** Enable request logging */
    enableLogging?: boolean;
    /** Request timeout in milliseconds */
    requestTimeout?: number;
}
/**
 * Default LLM settings for requests
 */
export interface DefaultLlmSettings {
    /** Default temperature value */
    temperature: number;
    /** Default maximum tokens */
    maxTokens: number;
    /** Default top_p value */
    topP: number;
    /** Default site URL for provider headers */
    siteUrl?: string;
    /** Default site name for provider headers */
    siteName?: string;
}
/**
 * Extended LLM settings that includes server-specific options
 */
export interface ExtendedLlmSettings extends LlmSettings {
    /** Request timeout override */
    timeout?: number;
    /** Custom headers for the request */
    customHeaders?: Record<string, string>;
    /** Enable debug logging for this request */
    debug?: boolean;
}
/**
 * Error types that can occur in the API
 */
export type ApiErrorType = 'invalid_request_error' | 'authentication_error' | 'api_error' | 'rate_limit_error' | 'server_error';
/**
 * Error codes for specific error conditions
 */
export type ApiErrorCode = 'model_not_found' | 'keys_exhausted' | 'validation_failed' | 'provider_error' | 'timeout_error' | 'rate_limited' | 'invalid_api_key' | 'insufficient_quota' | 'internal_error';
/**
 * Error response format compatible with OpenAI API
 */
export interface ErrorResponse {
    error: {
        /** Human-readable error message */
        message: string;
        /** The type of error */
        type: ApiErrorType;
        /** Specific error code */
        code: ApiErrorCode;
        /** Additional error details */
        details?: ErrorDetails;
    };
}
/**
 * Additional error details
 */
export interface ErrorDetails {
    /** Provider that caused the error */
    provider?: string;
    /** Status of API keys when error occurred */
    keyStatuses?: ApiKeyStatus[];
    /** Last error message from provider */
    lastError?: string;
    /** Request ID for debugging */
    requestId?: string;
    /** Timestamp when error occurred */
    timestamp?: string;
    /** Validation errors for specific fields */
    fieldErrors?: Record<string, string>;
}
/**
 * Custom API Error class with structured error information
 */
export declare class ApiError extends Error {
    readonly type: ApiErrorType;
    readonly code: ApiErrorCode;
    readonly details?: ErrorDetails;
    readonly statusCode: number;
    constructor(message: string, type: ApiErrorType, code: ApiErrorCode, statusCode?: number, details?: ErrorDetails);
    /**
     * Convert the error to an OpenAI-compatible error response
     */
    toResponse(): ErrorResponse;
    /**
     * Create a validation error
     */
    static validation(message: string, fieldErrors?: Record<string, string>): ApiError;
    /**
     * Create an authentication error
     */
    static authentication(message: string, provider?: string): ApiError;
    /**
     * Create a model not found error
     */
    static modelNotFound(model: string): ApiError;
    /**
     * Create a keys exhausted error
     */
    static keysExhausted(provider: string, keyStatuses: ApiKeyStatus[], lastError?: string): ApiError;
    /**
     * Create a rate limit error
     */
    static rateLimit(provider: string, retryAfter?: number): ApiError;
    /**
     * Create a timeout error
     */
    static timeout(provider: string, timeoutMs: number): ApiError;
}
/**
 * API Key status information for monitoring endpoint
 */
export interface KeyStatusInfo {
    /** Index of the key in the array */
    index: number;
    /** Current status of the key */
    status: ApiKeyStatus;
    /** Last time this key was used */
    lastUsed?: Date;
    /** Last error message for this key */
    lastError?: string;
    /** Number of successful requests with this key */
    successCount?: number;
    /** Number of failed requests with this key */
    failureCount?: number;
}
/**
 * Provider key status response
 */
export interface ProviderKeyStatus {
    /** Provider name */
    provider: string;
    /** Total number of keys for this provider */
    totalKeys: number;
    /** Currently active key index */
    currentKeyIndex: number;
    /** Status information for each key */
    keys: KeyStatusInfo[];
    /** Last rotation timestamp */
    lastRotation?: Date;
}
/**
 * Complete key status response for all providers
 */
export interface KeyStatusResponse {
    /** Status for each provider */
    providers: Record<string, ProviderKeyStatus>;
    /** Overall system status */
    systemStatus: 'healthy' | 'degraded' | 'critical';
    /** Timestamp of the status check */
    timestamp: string;
}
/**
 * Health check response
 */
export interface HealthCheckResponse {
    /** Overall status */
    status: 'ok' | 'error';
    /** Timestamp of the health check */
    timestamp: string;
    /** Server uptime in milliseconds */
    uptime?: number;
    /** Memory usage information */
    memory?: {
        used: number;
        total: number;
        percentage: number;
    };
    /** Version information */
    version?: string;
}
/**
 * Generic server response wrapper
 */
export interface ServerResponse<T = any> {
    /** Whether the request was successful */
    success: boolean;
    /** Response data (present on success) */
    data?: T;
    /** Error information (present on failure) */
    error?: ErrorResponse['error'];
    /** Request metadata */
    meta?: {
        requestId: string;
        timestamp: string;
        processingTime: number;
    };
}
/**
 * Request context information
 */
export interface RequestContext {
    /** Unique request identifier */
    requestId: string;
    /** Request start time */
    startTime: number;
    /** Client IP address */
    clientIp?: string;
    /** User agent string */
    userAgent?: string;
    /** Custom headers from the request */
    customHeaders?: Record<string, string>;
}
/**
 * Provider detection result
 */
export interface ProviderDetectionResult {
    /** Detected provider */
    provider: Provider | null;
    /** Confidence level of the detection (0-1) */
    confidence: number;
    /** Reason for the detection */
    reason: 'exact_match' | 'pattern_match' | 'fallback' | 'unknown';
    /** Alternative providers that could handle this model */
    alternatives: Provider[];
}
//# sourceMappingURL=types.d.ts.map