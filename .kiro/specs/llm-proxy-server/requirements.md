# Requirements Document

## Introduction

Создание простого HTTP сервера-прокси на TypeScript, который будет принимать запросы в формате OpenAI API на эндпоинт `/v1/chat/completions` и перенаправлять их к соответствующим LLM провайдерам через существующий модуль `llm_rotation.ts`. Сервер должен запускаться с помощью Bun и обеспечивать совместимость с OpenAI API для удобной интеграции с существующими клиентами.

## Requirements

### Requirement 1

**User Story:** Как разработчик, я хочу отправлять HTTP POST запросы на `/v1/chat/completions` в формате OpenAI API, чтобы получать ответы от различных LLM провайдеров через единый интерфейс.

#### Acceptance Criteria

1. WHEN пользователь отправляет POST запрос на `/v1/chat/completions` с валидными данными THEN сервер SHALL вернуть ответ в формате OpenAI API
2. WHEN в запросе указана модель THEN сервер SHALL автоматически определить соответствующего провайдера на основе названия модели
3. WHEN запрос содержит параметры temperature, max_tokens, top_p THEN сервер SHALL передать эти параметры в LlmManager
4. IF запрос не содержит обязательные поля (messages, model) THEN сервер SHALL вернуть ошибку 400 с описанием проблемы

### Requirement 2

**User Story:** Как администратор сервера, я хочу настраивать API ключи через переменные окружения, чтобы безопасно управлять доступом к различным LLM провайдерам.

#### Acceptance Criteria

1. WHEN сервер запускается THEN он SHALL читать API ключи из переменных окружения для всех поддерживаемых провайдеров
2. WHEN API ключ для провайдера не найден THEN сервер SHALL продолжить работу, но запросы к этому провайдеру будут возвращать ошибку
3. WHEN используется модель, для которой нет API ключа THEN сервер SHALL вернуть понятную ошибку пользователю

### Requirement 3

**User Story:** Как клиент API, я хочу получать список доступных моделей через эндпоинт `/v1/models`, чтобы знать какие модели поддерживает сервер.

#### Acceptance Criteria

1. WHEN пользователь отправляет GET запрос на `/v1/models` THEN сервер SHALL вернуть список всех доступных моделей в формате OpenAI API
2. WHEN модель принадлежит определенному провайдеру THEN в ответе SHALL быть указан владелец модели (owned_by)
3. WHEN список моделей формируется THEN он SHALL основываться на конфигурации из LlmManager.modelConfigurations

### Requirement 4

**User Story:** Как DevOps инженер, я хочу мониторить состояние сервера через эндпоинт `/health`, чтобы обеспечить надежность системы.

#### Acceptance Criteria

1. WHEN пользователь отправляет GET запрос на `/health` THEN сервер SHALL вернуть статус "ok" и текущее время
2. WHEN сервер работает корректно THEN health check SHALL возвращать HTTP статус 200
3. WHEN происходит критическая ошибка THEN health check SHALL отражать проблемное состояние

### Requirement 5

**User Story:** Как разработчик, я хочу запускать сервер с помощью Bun, чтобы использовать современную и быструю JavaScript runtime.

#### Acceptance Criteria

1. WHEN сервер написан на TypeScript THEN он SHALL корректно компилироваться и запускаться через Bun
2. WHEN сервер запускается THEN он SHALL выводить информацию о доступных эндпоинтах и настройках
3. WHEN указан порт через переменную окружения PORT THEN сервер SHALL использовать этот порт, иначе порт 3000

### Requirement 6

**User Story:** Как пользователь API, я хочу получать детальные сообщения об ошибках, чтобы понимать причину неудачных запросов.

#### Acceptance Criteria

1. WHEN происходит ошибка валидации запроса THEN сервер SHALL вернуть HTTP 400 с описанием проблемы
2. WHEN происходит ошибка в LlmManager THEN сервер SHALL вернуть HTTP 500 с безопасным сообщением об ошибке
3. WHEN модель не поддерживается THEN сервер SHALL вернуть HTTP 400 с сообщением о неподдерживаемой модели
4. WHEN все API ключи провайдера исчерпаны THEN сервер SHALL вернуть понятную ошибку о недоступности сервиса